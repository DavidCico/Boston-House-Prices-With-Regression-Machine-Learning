{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Machine Learning Case Study: Boston housing price\n",
    "\n",
    "<p style=\"text-align: justify\">How do you work through a predictive modeling machine learning problem end-to-end? In this notebook, we work through a case study regression predictive modeling problem in Python including each step of the applied machine learning process. This project covers the following aspects:</p>\n",
    "\n",
    "<ul>\n",
    "<li>How to work through a regression predictive modeling problem end-to-end.</li>\n",
    "<li>How to use data transforms to improve model performance.</li>\n",
    "<li>How to use algorithm tuning to improve model performance.</li>\n",
    "<li>How to use ensemble methods and tuning of ensemble methods to improve model performance.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">For this project we will investigate the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\">Boston House Price</a> dataset. Each record in the database\n",
    "describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are defined as follows (taken from the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\">UCI Machine Learning Repository</a>):</p>\n",
    "\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centers\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: $1000(Bk - 0:63)^2$ where $Bk$ is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in \\$1000s\n",
    "\n",
    "We can see that the input attributes have a mixture of units, which may require normalization of the features.\n",
    "\n",
    "## Load the dataset \n",
    "\n",
    "Let's start off by loading the libraries required for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EC-PM-3\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy\n",
    "from numpy import arange\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can now load the dataset that we downloaded from the UCI Machine Learning repository website.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filename = 'housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "dataset = read_csv(filename, delim_whitespace=True, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The short names are specified for each attribute so that we can reference\n",
    "them clearly later. We can see that attributes are delimited by whitespace rather than commas in this file, and we indicate this to <em>read_csv()</em> function via the \"delim_whitespace\" argument. We now have our data loaded.</p>\n",
    "\n",
    "## Analyze data\n",
    "\n",
    "<p style=\"text-align: justify\">We can now take a closer look at our loaded data. Let's start off by confirming the dimensions of the dataset, e.g. the number of rows and columns. We have 506 instances to work with and can con\f",
    "rm the data has 14 attributes including the output attribute MEDV.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "#shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's also look at the data types of each attribute. We can see that all of the attributes are numeric, mostly real values (float) and some have been interpreted as integers (int).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM       float64\n",
      "ZN         float64\n",
      "INDUS      float64\n",
      "CHAS         int64\n",
      "NOX        float64\n",
      "RM         float64\n",
      "AGE        float64\n",
      "DIS        float64\n",
      "RAD          int64\n",
      "TAX        float64\n",
      "PTRATIO    float64\n",
      "B          float64\n",
      "LSTAT      float64\n",
      "MEDV       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# types\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's now take a peek at the first 20 rows of the data. We can confirm that the scales for the attributes are all over the place because of the differing units. We may benefit from some transforms later on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
      "0   0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n",
      "1   0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n",
      "2   0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n",
      "3   0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n",
      "4   0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n",
      "5   0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n",
      "6   0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n",
      "7   0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n",
      "8   0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n",
      "9   0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n",
      "10  0.22489  12.5   7.87     0  0.524  6.377   94.3  6.3467    5  311.0   \n",
      "11  0.11747  12.5   7.87     0  0.524  6.009   82.9  6.2267    5  311.0   \n",
      "12  0.09378  12.5   7.87     0  0.524  5.889   39.0  5.4509    5  311.0   \n",
      "13  0.62976   0.0   8.14     0  0.538  5.949   61.8  4.7075    4  307.0   \n",
      "14  0.63796   0.0   8.14     0  0.538  6.096   84.5  4.4619    4  307.0   \n",
      "15  0.62739   0.0   8.14     0  0.538  5.834   56.5  4.4986    4  307.0   \n",
      "16  1.05393   0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307.0   \n",
      "17  0.78420   0.0   8.14     0  0.538  5.990   81.7  4.2579    4  307.0   \n",
      "18  0.80271   0.0   8.14     0  0.538  5.456   36.6  3.7965    4  307.0   \n",
      "19  0.72580   0.0   8.14     0  0.538  5.727   69.5  3.7965    4  307.0   \n",
      "\n",
      "    PTRATIO       B  LSTAT  MEDV  \n",
      "0      15.3  396.90   4.98  24.0  \n",
      "1      17.8  396.90   9.14  21.6  \n",
      "2      17.8  392.83   4.03  34.7  \n",
      "3      18.7  394.63   2.94  33.4  \n",
      "4      18.7  396.90   5.33  36.2  \n",
      "5      18.7  394.12   5.21  28.7  \n",
      "6      15.2  395.60  12.43  22.9  \n",
      "7      15.2  396.90  19.15  27.1  \n",
      "8      15.2  386.63  29.93  16.5  \n",
      "9      15.2  386.71  17.10  18.9  \n",
      "10     15.2  392.52  20.45  15.0  \n",
      "11     15.2  396.90  13.27  18.9  \n",
      "12     15.2  390.50  15.71  21.7  \n",
      "13     21.0  396.90   8.26  20.4  \n",
      "14     21.0  380.02  10.26  18.2  \n",
      "15     21.0  395.62   8.47  19.9  \n",
      "16     21.0  386.85   6.58  23.1  \n",
      "17     21.0  386.75  14.67  17.5  \n",
      "18     21.0  288.99  11.69  20.2  \n",
      "19     21.0  390.95  11.28  18.2  \n"
     ]
    }
   ],
   "source": [
    "#head\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's summarize the distribution of each attribute. We now have a better feeling for how different the attributes are. The min and max values as well are the means vary a lot. We are likely going to get better results by rescaling the data in some way. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CRIM     ZN  INDUS     CHAS    NOX     RM    AGE    DIS    RAD  \\\n",
      "count  5.1e+02  506.0  506.0  5.1e+02  506.0  506.0  506.0  506.0  506.0   \n",
      "mean   3.6e+00   11.4   11.1  6.9e-02    0.6    6.3   68.6    3.8    9.5   \n",
      "std    8.6e+00   23.3    6.9  2.5e-01    0.1    0.7   28.1    2.1    8.7   \n",
      "min    6.3e-03    0.0    0.5  0.0e+00    0.4    3.6    2.9    1.1    1.0   \n",
      "25%    8.2e-02    0.0    5.2  0.0e+00    0.4    5.9   45.0    2.1    4.0   \n",
      "50%    2.6e-01    0.0    9.7  0.0e+00    0.5    6.2   77.5    3.2    5.0   \n",
      "75%    3.7e+00   12.5   18.1  0.0e+00    0.6    6.6   94.1    5.2   24.0   \n",
      "max    8.9e+01  100.0   27.7  1.0e+00    0.9    8.8  100.0   12.1   24.0   \n",
      "\n",
      "         TAX  PTRATIO      B  LSTAT   MEDV  \n",
      "count  506.0    506.0  506.0  506.0  506.0  \n",
      "mean   408.2     18.5  356.7   12.7   22.5  \n",
      "std    168.5      2.2   91.3    7.1    9.2  \n",
      "min    187.0     12.6    0.3    1.7    5.0  \n",
      "25%    279.0     17.4  375.4    6.9   17.0  \n",
      "50%    330.0     19.1  391.4   11.4   21.2  \n",
      "75%    666.0     20.2  396.2   17.0   25.0  \n",
      "max    711.0     22.0  396.9   38.0   50.0  \n"
     ]
    }
   ],
   "source": [
    "# descriptions\n",
    "set_option('precision', 1)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Now, let's now take a look at the correlation between all of the numeric attributes. This is interesting. We can see that many of the attributes have a strong correlation (e.g. > 0.70 or < -0.70). For example:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>NOX and INDUS with 0.77.</li>\n",
    "    <li>DIS and INDUS with -0.71.</li>\n",
    "    <li>TAX and INDUS with 0.72.</li>\n",
    "    <li>AGE and NOX with 0.73.</li>\n",
    "    <li>DIS and NOX with -0.78.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">It also looks like LSTAT has a good negative correlation with the output variable MEDV with a value of -0.74.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CRIM    ZN  INDUS      CHAS   NOX    RM   AGE   DIS       RAD   TAX  \\\n",
      "CRIM     1.00 -0.20   0.41 -5.59e-02  0.42 -0.22  0.35 -0.38  6.26e-01  0.58   \n",
      "ZN      -0.20  1.00  -0.53 -4.27e-02 -0.52  0.31 -0.57  0.66 -3.12e-01 -0.31   \n",
      "INDUS    0.41 -0.53   1.00  6.29e-02  0.76 -0.39  0.64 -0.71  5.95e-01  0.72   \n",
      "CHAS    -0.06 -0.04   0.06  1.00e+00  0.09  0.09  0.09 -0.10 -7.37e-03 -0.04   \n",
      "NOX      0.42 -0.52   0.76  9.12e-02  1.00 -0.30  0.73 -0.77  6.11e-01  0.67   \n",
      "RM      -0.22  0.31  -0.39  9.13e-02 -0.30  1.00 -0.24  0.21 -2.10e-01 -0.29   \n",
      "AGE      0.35 -0.57   0.64  8.65e-02  0.73 -0.24  1.00 -0.75  4.56e-01  0.51   \n",
      "DIS     -0.38  0.66  -0.71 -9.92e-02 -0.77  0.21 -0.75  1.00 -4.95e-01 -0.53   \n",
      "RAD      0.63 -0.31   0.60 -7.37e-03  0.61 -0.21  0.46 -0.49  1.00e+00  0.91   \n",
      "TAX      0.58 -0.31   0.72 -3.56e-02  0.67 -0.29  0.51 -0.53  9.10e-01  1.00   \n",
      "PTRATIO  0.29 -0.39   0.38 -1.22e-01  0.19 -0.36  0.26 -0.23  4.65e-01  0.46   \n",
      "B       -0.39  0.18  -0.36  4.88e-02 -0.38  0.13 -0.27  0.29 -4.44e-01 -0.44   \n",
      "LSTAT    0.46 -0.41   0.60 -5.39e-02  0.59 -0.61  0.60 -0.50  4.89e-01  0.54   \n",
      "MEDV    -0.39  0.36  -0.48  1.75e-01 -0.43  0.70 -0.38  0.25 -3.82e-01 -0.47   \n",
      "\n",
      "         PTRATIO     B  LSTAT  MEDV  \n",
      "CRIM        0.29 -0.39   0.46 -0.39  \n",
      "ZN         -0.39  0.18  -0.41  0.36  \n",
      "INDUS       0.38 -0.36   0.60 -0.48  \n",
      "CHAS       -0.12  0.05  -0.05  0.18  \n",
      "NOX         0.19 -0.38   0.59 -0.43  \n",
      "RM         -0.36  0.13  -0.61  0.70  \n",
      "AGE         0.26 -0.27   0.60 -0.38  \n",
      "DIS        -0.23  0.29  -0.50  0.25  \n",
      "RAD         0.46 -0.44   0.49 -0.38  \n",
      "TAX         0.46 -0.44   0.54 -0.47  \n",
      "PTRATIO     1.00 -0.18   0.37 -0.51  \n",
      "B          -0.18  1.00  -0.37  0.33  \n",
      "LSTAT       0.37 -0.37   1.00 -0.74  \n",
      "MEDV       -0.51  0.33  -0.74  1.00  \n"
     ]
    }
   ],
   "source": [
    "# correlation\n",
    "set_option('precision', 2)\n",
    "print(dataset.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "<p style=\"text-align: justify\">Let's look at visualizations of individual attributes. It is often useful to look at your data\n",
    "using multiple different visualizations in order to spark ideas. Let's look at histograms of each attribute to get a sense of the data distributions.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can see that some attributes may have an exponential distribution, such as CRIM, ZN, AGE and B. We can see that others may have a bimodal distribution such as RAD and TAX.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "dataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(14,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's look at the same distributions using density plots that smooth them out a bit.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">This perhaps adds more evidence to our suspicion about possible exponential and bimodal\n",
    "distributions. It also looks like NOX, RM and LSTAT may be skewed Gaussian distributions, which might be helpful later with transforms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density plots\n",
    "dataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=1, figsize=(14,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's look at the data with box and whisker plots of each attribute.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">This helps point out the skew in many distributions so much so that data looks like outliers (e.g. beyond the whisker of the plots).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box whisker plots\n",
    "dataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, fontsize=12, figsize =(14,14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's look at some visualizations of the interactions between variables. The best place to start\n",
    "is a scatter plot matrix.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can see that some of the higher correlated attributes do show good structure in their\n",
    "relationship. Not linear, but nice predictable curved relationships.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot matrix\n",
    "scatter_matrix(dataset, figsize = (14,14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's also visualize the correlations between the attributes.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The dark red color shows positive correlation whereas the dark blue color shows negative correlation. We can also see some dark red and dark blue that suggest candidates for removal to better improve accuracy of models later on.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none',cmap=cm.coolwarm)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,14,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">There is a lot of structure in this dataset. We need to think about transforms that we could use\n",
    "later to better expose the structure which in turn may improve modeling accuracy. So far it would be worth trying:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Feature selection and removing the most correlated attributes.</li>\n",
    "    <li>Normalizing the dataset to reduce the effect of differing scales.</li>\n",
    "    <li>Standardizing the dataset to reduce the effects of differing distributions.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">With lots of additional time I would also explore the possibility of binning (discretization)\n",
    "of the data. This can often improve accuracy for decision tree algorithms.</p>\n",
    "\n",
    "## Validation dataset\n",
    "\n",
    "It is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset\n",
    "for modeling and hold back 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Baseline\n",
    "\n",
    "<p style=\"text-align: justify\">We have no idea what algorithms will do well on this problem. Gut feel suggests regression algorithms like Linear Regression and ElasticNet may do well. It is also possible that decision trees and even SVM may do well. With no proper idea in mind, let's design our test harness. We will use a <b>10-fold cross validation</b>. The dataset is not too small and this is a good standard test harness configuration. We will evaluate algorithms using the <b>Mean Squared Error (MSE)</b> metric. MSE will give a gross idea of how wrong all predictions are (0 is perfect).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Let's create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this regression problem. The six algorithms selected include:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Linear Algorithms</b>: Linear Regression (LR), Lasso Regression (LASSO) and ElasticNet (EN).</li>\n",
    "    <li><b>Nonlinear Algorithms</b>: Classification and Regression Trees (CART), Support Vector Regression (SVR) and <em>k</em>-Nearest Neighbors (KNN). </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVR', SVR()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The algorithms all use default tuning parameters. Let's compare the algorithms. We will\n",
    "display the mean and standard deviation of MSE for each algorithm as we calculate it and collect the results for use later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">It looks like LR has the lowest MSE, followed closely by CART.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Let's take a look at the distribution of scores across all cross validation folds by algorithm.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can see similar distributions for the regression algorithms and perhaps a tighter distribution of scores for CART.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison', y=0.92)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(9, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The different scales of the data is probably hurting the skill of all of the algorithms, and\n",
    "perhaps more so for SVR and KNN. In the next section we will look at running the same algorithms using a standardized copy of the data.</p>\n",
    "\n",
    "## Evaluate algorithms: Standardization\n",
    "\n",
    "<p style=\"text-align: justify\">We suspect that the differing scales of the raw data may be negatively impacting the skill of some of the algorithms. Let's evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of 1. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize dataset\n",
    "pipelines = []\n",
    "\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "\n",
    "pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\n",
    "\n",
    "#Re-evaluate standardized data\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the example provides a list of mean squared errors. We can see that scaling did\n",
    "have an effect on KNN, driving the error lower than the other models.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Let's take a look at the distribution of the scores across the cross validation folds. We can see that KNN has both a tight distribution of error and has the lowest score.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison', y=0.92)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(9, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Results With Tuning\n",
    "\n",
    "<p style=\"text-align: justify\">We know from the results in the previous section that KNN achieves good results on a scaled version of the dataset. But can it do better. The default value for the number of neighbors in KNN is 7. We can use a grid search to try a set of different numbers of neighbors and see if we can improve the score. The below example tries odd <em>k</em> values from 1 to 21, an arbitrary range covering a known good value of 7. Each <em>k</em> value (\"n_neighbors\") is evaluated using 10-fold cross validation on a standardized copy of the training dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Algorithm tuning\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "k_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\n",
    "param_grid = dict(n_neighbors=k_values)\n",
    "model = KNeighborsRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can display the mean and standard deviation scores as well as the best performing value\n",
    "for k below.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can see that the best for k (\"n_neighbors\") is 3 providing a mean squared error of\n",
    "-18.172137, the best so far.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "<p style=\"text-align: justify\">Another way that we can improve the performance of algorithms on this problem is by using ensemble methods. In this section we will evaluate four different ensemble machine learning algorithms, two boosting and two bagging methods:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Boosting Methods</b>: AdaBoost (AB) and Gradient Boosting (GBM).</li>\n",
    "    <li><b>Bagging Methods</b>: Random Forests (RF) and Extra Trees (ET).</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">We will use the same test harness as before, 10-fold cross validation and pipelines that standardize the training data for each fold.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembles\n",
    "ensembles = []\n",
    "\n",
    "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\n",
    "\n",
    "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "\n",
    "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\n",
    "\n",
    "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name,model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the example calculates the mean squared error for each method using the default parameters. We can see that we're generally getting better scores than our linear and nonlinear algorithms in previous sections.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can also plot the distribution of scores across the cross validation folds.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Ensemble Algorithm Comparison', y=0.92)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(9, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">It looks like Gradient Boosting has a better mean score, it also looks like Extra Trees has a similar distribution and perhaps a better median score.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can probably do better, given that the ensemble techniques used the default parameters. In the next section we will look at tuning the Gradient Boosting to further lift the performance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Ensemble Methods\n",
    "\n",
    "<p style=\"text-align: justify\">The default number of boosting stages to perform (\"n_estimators\") is 100. This is a good candidate parameter of Gradient Boosting to tune. Often, the larger the number of boosting stages, the better the performance but the longer the training time. In this section, we will look at tuning the number of stages for gradient boosting. Below, we define a parameter grid \"n_estimators\" values from 50 to 400 in increments of 50. Each setting is evaluated using 10-fold\n",
    "cross validation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled GBM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\n",
    "model = GradientBoostingRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">As before, we can summarize the best configuration and get an idea of how performance\n",
    "changed with each different configuration.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can see that the best configuration is \"n_estimators\"=400 resulting in a mean squared\n",
    "error of -9.356471, about 0.65 units better than the untuned method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Model\n",
    "\n",
    "<p style=\"text-align: justify\">In this section we will finalize the gradient boosting model and evaluate it on our hold out validation dataset. First we need to prepare the model and train it on the entire training dataset. This includes standardizing the training dataset before training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = GradientBoostingRegressor(random_state=seed, n_estimators=400)\n",
    "model.fit(rescaledX, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can then scale the inputs for the validation dataset and generate predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the validation dataset\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(mean_squared_error(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can see that the estimated mean squared error is 11.8, close to our estimate of -9.3.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
